{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***Kindly make a copy of this notebook and start experimenting.***"
      ],
      "metadata": {
        "id": "PBnflA5KWvRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the github repo"
      ],
      "metadata": {
        "id": "6lSRO7G_bjW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sgandhi31/LPRNet_Optimization.git\n",
        "%cd LPRNet_Optimization\n",
        "\n",
        "# Check files and structure\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDykaD7QbfUD",
        "outputId": "b8be8ae2-b47e-4fee-9571-e4ea9d8770a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_Optimization'...\n",
            "remote: Enumerating objects: 1116, done.\u001b[K\n",
            "remote: Counting objects: 100% (1116/1116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1108/1108), done.\u001b[K\n",
            "remote: Total 1116 (delta 46), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1116/1116), 4.09 MiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/LPRNet_Optimization\n",
            "data\t mlc\tREADME.md\t  test_LPRNetOpt_mlc.py  test_LPRNet_quantize.py  tuning_log.json\n",
            "LICENSE  model\trequirements.txt  test_LPRNet.py\t train_LPRNet.py\t  weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Python libraries\n",
        "!pip install torch torchvision opencv-python-headless imutils pillow numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VOREWCHKZwHj",
        "outputId": "7685fe78-04a9-41f2-b5ff-247fae2c95cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check PyTorch and GPU setup\n",
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE19-kxiZ1xI",
        "outputId": "58012245-2f94-46b4-f0f6-4d41a3dc2da4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.5.1+cu121\n",
            "GPU Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the original model"
      ],
      "metadata": {
        "id": "NYxFao8Fc9VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test script to verify setup\n",
        "\n",
        "!python test_LPRNet.py --test_img_dirs 'data/test' --pretrained_model 'weights/Final_LPRNet_model.pth'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGMl9zeyZ54k",
        "outputId": "90eea64e-b8cd-440d-8f6d-9c373b461fc6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Optimization/LPRNet_Optimization/LPRNet_Optimization/test_LPRNet.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location = torch.device('cpu')))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.899 [899:61:40:1000]\n",
            "[Info] Test Speed: 0.21213060808181763s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get model size of original model"
      ],
      "metadata": {
        "id": "DfYokZY2dDBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime onnxruntime-tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_dt8G0QdEqF",
        "outputId": "58f296d8-1231-4b5f-b17c-97d3c39484d8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnxruntime-tools\n",
            "  Downloading onnxruntime_tools-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting onnx (from onnxruntime-tools)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (9.0.0)\n",
            "Collecting py3nvml (from onnxruntime-tools)\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting xmltodict (from py3nvml->onnxruntime-tools)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_tools-1.7.0-py3-none-any.whl (212 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict, onnx, humanfriendly, py3nvml, coloredlogs, onnxruntime-tools, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.1 onnxruntime-tools-1.7.0 py3nvml-0.2.7 xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model.LPRNet import build_lprnet\n",
        "\n",
        "# Load pretrained model\n",
        "model = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "model.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94)  # Input size for LPRNet\n",
        "torch.onnx.export(model, dummy_input, \"/content/LPRNet_Optimization/LPRNet.onnx\", verbose=True)\n",
        "print(\"Model exported to ONNX format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gde4tcZtaRJD",
        "outputId": "02dd9beb-282c-4812-80ed-70ba07fdd639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e1b03eec9997>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to ONNX format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ONNX file size\n",
        "import os\n",
        "onnx_file = \"/content/LPRNet_Optimization/LPRNet.onnx\"\n",
        "size_mb = os.path.getsize(onnx_file) / (1024 * 1024)\n",
        "print(f\"ONNX Model Size: {size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLaShIHjcyDm",
        "outputId": "8bb105e4-710a-4ffe-a99c-c400233f4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Model Size: 1.91 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Original Model:\n",
        "\n",
        "1. Size: 1.91 MB\n",
        "\n",
        "2. Test Accuracy: 90.3%\n",
        "\n",
        "3. Inference Time: 0.2 seconds"
      ],
      "metadata": {
        "id": "8RYB0o8eddCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimization 1 - PTQ\n"
      ],
      "metadata": {
        "id": "bgoz7j2zeGTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test the script\n",
        "\n",
        "!python test_LPRNet_quantize.py --test_img_dirs 'data/test' --quantized_model 'weights/Final_LPRNet_model.pth'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0DQxEE0iAZv",
        "outputId": "817ea118-58f9-4ed4-d2cd-650a78f8665e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_Optimization/test_LPRNet_quantize.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.78 [780:148:72:1000]\n",
            "[Info] Test Speed: 0.027929948091506958s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_mb = os.path.getsize(\"/content/LPRNet_Optimization/weights/Quantized_LPRNet_model.pth\") / (1024 * 1024)\n",
        "print(f\"Quantized Model Size: {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0lIXLEpjMiP",
        "outputId": "d1fe3db8-e27d-44d2-f2d5-16fc070e9a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model Size: 0.51 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimized Model (Post Training Quantization):\n",
        "\n",
        "1. Size: 0.51 MB\n",
        "\n",
        "2. Test Accuracy: 78%\n",
        "\n",
        "3. Inference Time: 0.027 Seconds\n"
      ],
      "metadata": {
        "id": "XN-Jni87dvwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n30bOyEaDTFb",
        "outputId": "7993d1eb-c738-4619-f34c-51ae16eac6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.1.0.dev20241207-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (4.25.5)\n",
            "Downloading onnxscript-0.1.0.dev20241207-py3-none-any.whl (725 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.8/725.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxscript\n",
            "Successfully installed onnxscript-0.1.0.dev20241207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Quantized model Test In Notebook"
      ],
      "metadata": {
        "id": "cC6sE5U5Ff3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "from torch.ao.quantization import QuantStub, DeQuantStub\n",
        "\n",
        "# Define a wrapper to add QuantStub and DeQuantStub\n",
        "class QuantizedLPRNet(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(QuantizedLPRNet, self).__init__()\n",
        "        self.quant = QuantStub()  # Handles input quantization\n",
        "        self.model = model\n",
        "        self.dequant = DeQuantStub()  # Handles output dequantization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.model(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "def load_pretrained_model(model_path):\n",
        "    # Load the pretrained LPRNet model\n",
        "    model = build_lprnet_quantize(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def apply_quantization(model, calibration_data):\n",
        "    # Wrap the model with QuantStub and DeQuantStub\n",
        "    model = QuantizedLPRNet(model)\n",
        "\n",
        "    # Define the quantization configuration\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "    # Prepare the model for quantization\n",
        "    model_prepared = torch.ao.quantization.prepare(model, inplace = False)\n",
        "\n",
        "    # Calibrate the model using a representative dataset\n",
        "    model_prepared.eval()\n",
        "    with torch.no_grad():\n",
        "      model_prepared(calibration_data)\n",
        "\n",
        "    # Convert to quantized version\n",
        "    model_quantized = torch.ao.quantization.convert(model_prepared, inplace = False)\n",
        "    return model_quantized\n",
        "\n",
        "\n",
        "def save_pth_model(model_path, save_path):\n",
        "  # # Load your trained model\n",
        "  model_path = 'weights/Final_LPRNet_model.pth'\n",
        "  model = load_pretrained_model(model_path)\n",
        "\n",
        "  # # Example calibration data (replace with your actual calibration data)\n",
        "  calibration_data = torch.randn(1, 3, 24, 94) # Dummy data\n",
        "\n",
        "  # # Apply quantization\n",
        "  quantized_model = apply_quantization(model, calibration_data)\n",
        "\n",
        "  # # Save the quantized model\n",
        "  torch.save(quantized_model.state_dict(), savepath)\n",
        "\n",
        "def get_model_size(model_path):\n",
        "  size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "  print(f\"Model Size: {size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Quantization applied and model saved.\")\n"
      ],
      "metadata": {
        "id": "oBHg04fJ5HFN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "# from model.LPRNet import build_lprnet\n",
        "# import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
        "\n",
        "test_img_dirs = os.path.expanduser('/content/LPRNet_Pytorch/data/test')\n",
        "datasets = LPRDataLoader(test_img_dirs.split(','), [94, 24], 8)\n",
        "\n",
        "epoch_size = len(datasets) // 100\n",
        "batch_iterator = iter(DataLoader(datasets, 100, shuffle=True, num_workers=8, collate_fn=collate_fn))\n",
        "#calibration:\n",
        "model_path = 'weights/Final_LPRNet_model.pth'\n",
        "new_model = load_pretrained_model(model_path)\n",
        "calibration_data,_,_ = next(iter(DataLoader(datasets,100, shuffle = True, collate_fn = collate_fn)))\n",
        "quantized_model = apply_quantization(new_model, calibration_data)\n",
        "\n",
        "\n",
        "Tp = 0\n",
        "Tn_1 = 0\n",
        "Tn_2 = 0\n",
        "t1 = time.time()\n",
        "for i in range(epoch_size):\n",
        "    # load train data\n",
        "    images, labels, lengths = next(batch_iterator)\n",
        "    start = 0\n",
        "    targets = []\n",
        "    for length in lengths:\n",
        "        label = labels[start:start+length]\n",
        "        targets.append(label)\n",
        "        start += length\n",
        "    targets = np.array([el.numpy() for el in targets])\n",
        "    imgs = images.numpy().copy()\n",
        "\n",
        "    images = Variable(images)\n",
        "    # new_model = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "    # calibration_data = torch.randn(1, 3, 24, 94) # Dummy data\n",
        "    # quantized_model = apply_quantization(new_model, calibration_data)\n",
        "\n",
        "    # quantized_model = QuantizedLPRNet(new_model)\n",
        "    # quantized_model.load_state_dict(torch.load(\"Quantized_LPRNet_model.pth\", map_location = torch.device('cpu')))\n",
        "    # Apply quantization\n",
        "\n",
        "    prebs = quantized_model(images)\n",
        "    # greedy decode\n",
        "    prebs = prebs.cpu().detach().numpy()\n",
        "    preb_labels = list()\n",
        "    for i in range(prebs.shape[0]):\n",
        "        preb = prebs[i, :, :]\n",
        "        preb_label = list()\n",
        "        for j in range(preb.shape[1]):\n",
        "            preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "        no_repeat_blank_label = list()\n",
        "        pre_c = preb_label[0]\n",
        "        if pre_c != len(CHARS) - 1:\n",
        "            no_repeat_blank_label.append(pre_c)\n",
        "        for c in preb_label: # dropout repeate label and blank label\n",
        "            if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                if c == len(CHARS) - 1:\n",
        "                    pre_c = c\n",
        "                continue\n",
        "            no_repeat_blank_label.append(c)\n",
        "            pre_c = c\n",
        "        preb_labels.append(no_repeat_blank_label)\n",
        "    for i, label in enumerate(preb_labels):\n",
        "        if len(label) != len(targets[i]):\n",
        "            Tn_1 += 1\n",
        "            continue\n",
        "        if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "            Tp += 1\n",
        "        else:\n",
        "            Tn_2 += 1\n",
        "Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "t2 = time.time()\n",
        "print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNa1yk5yKQpz",
        "outputId": "6555a592-035f-4f39-a77c-5b90501fa16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-102-f9d4bd70b59e>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.78 [780:129:91:1000]\n",
            "[Info] Test Speed: 0.023357553005218506s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), 'Quantized_LPRNet_model.pth')\n",
        "\n",
        "print(\"Quantization applied and model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98_Ubj3cV6pJ",
        "outputId": "5be7fd6d-4126-45b2-e33e-b78fa68f11a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization applied and model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimization - Unstrcutured Pruning"
      ],
      "metadata": {
        "id": "6p5omtIwyM4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adjust the pruning ratio to experiement with different versions of pruned models."
      ],
      "metadata": {
        "id": "djldXBI8It84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 model/LPRNet_pruned.py --pretrained_model '/content/LPRNet_Optimization/weights/Final_LPRNet_model.pth' --pruning_ratio 0.5 --verbose True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNs7YehMFqoG",
        "outputId": "cf555e8f-b6ab-4aee-e931-34181a253c72"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_Optimization/model/LPRNet_pruned.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
            "LPRNet model loaded successfully!\n",
            "model has been pruned\n",
            "No. of Non-Zero Parameters: 446975\n",
            "Pruned model with ratio 0.5:\n",
            "No. of Non-Zero Parameters: 225304\n",
            "Parameter reduction: 49.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test the pruned model"
      ],
      "metadata": {
        "id": "116cwiypI2zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_LPRNet.py --test_img_dirs 'data/test' --pretrained_model '/content/LPRNet_Optimization/weights/unstructured_pruned_LPRNet_model_0.5.pth'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTcgnGFCyQfF",
        "outputId": "16d3a20d-b1f3-4b89-d758-75e02c3aa900"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Optimization/LPRNet_Optimization/LPRNet_Optimization/test_LPRNet.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location = torch.device('cpu')))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.825 [825:98:77:1000]\n",
            "[Info] Test Speed: 0.15195665550231935s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pruned model:\n",
        "\n",
        "1. size : 1.91 MB\n",
        "2. Accuracy: 82.5%\n",
        "3. Inference Time: 0.15 seconds\n"
      ],
      "metadata": {
        "id": "mFtxZc3w3jm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned + Quantized"
      ],
      "metadata": {
        "id": "i0kzRb9v3Q50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test script to verify setup\n",
        "\n",
        "!python test_LPRNet_quantize.py --test_img_dirs 'data/test' --quantized_model '/content/LPRNet_Optimization/weights/unstructured_pruned_LPRNet_model_0.5.pth'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR3Yidxd3WX2",
        "outputId": "7340c239-c933-4165-901c-6b3bede4341e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_Optimization/LPRNet_Optimization/LPRNet_Optimization/test_LPRNet_quantize.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.625 [625:217:158:1000]\n",
            "[Info] Test Speed: 0.023930617094039916s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_model(modelObj, model_path):\n",
        "    # Load the pretrained LPRNet model\n",
        "    modelObj.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    modelObj.eval()\n",
        "    return modelObj\n",
        "\n",
        "def save_pth_model(model,model_path, save_path):\n",
        "  model = load_pretrained_model(model, model_path)\n",
        "  # # Example calibration data (replace with your actual calibration data)\n",
        "  calibration_data = torch.randn(1, 3, 24, 94) # Dummy data\n",
        "\n",
        "  # # Apply quantization\n",
        "  quantized_model = apply_quantization(model, calibration_data)\n",
        "\n",
        "  # # Save the quantized model\n",
        "  torch.save(quantized_model.state_dict(), save_path)\n",
        "\n",
        "def get_model_size(model_path):\n",
        "  size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "  print(f\"Model Size: {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "azFohsCR6LQs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.LPRNet_quantize import build_lprnet_quantize\n",
        "import os\n",
        "\n",
        "model = build_lprnet_quantize(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "save_pth_model(model, '/content/LPRNet_Optimization/weights/unstructured_pruned_LPRNet_model_0.5.pth', 'weights/pruned_quantized_LPRNet_model.pth')\n",
        "get_model_size('weights/pruned_quantized_LPRNet_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHX9obyL7uw7",
        "outputId": "94c5a77f-7748-40d0-a6e1-71887f1f63b6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-c03a5d0fa733>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelObj.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 0.51 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLC optimization"
      ],
      "metadata": {
        "id": "kDoqnb4wuhU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m  pip install mlc-ai-cpu -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMeKvWK7ukph",
        "outputId": "8af5310f-3a41-4742-fa7d-e2501d31e933"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-cpu\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_cpu-0.17.2-cp310-cp310-manylinux_2_28_x86_64.whl (185.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.12.2)\n",
            "Installing collected packages: mlc-ai-cpu\n",
            "Successfully installed mlc-ai-cpu-0.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autotuning"
      ],
      "metadata": {
        "id": "Y1iIg8ivCt6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directly check the optimized model"
      ],
      "metadata": {
        "id": "W2KEeYbRgg8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test script to verify setup\n",
        "\n",
        "!python test_LPRNetOpt_mlc.py --test_img_dirs 'data/test' --optimized_model 'weights/Final_LPRNet_model.pth' --log_file '/content/LPRNet_Pytorch/tuning_log.json'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS_3YPtqggmc",
        "outputId": "c90ca973-539a-4646-e968-d1a366b13570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Compile...\n",
            "/content/LPRNet_Optimization/mlc/load_autotune.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(model_path, map_location = torch.device('cpu')))\n",
            "/content/LPRNet_Pytorch/tuning_log.json does not exist!\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1d64080c9dd43d925442e0ac42af9f21\", [1, 1, 24, 94, 3], [16, 1, 3, 3, 3, 4], [1, 16, 1, 1, 4], [1, 16, 1, 1, 4], [1, 16, 22, 92, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 24, 94, 3]\n",
            "p1 = PLACEHOLDER [16, 1, 3, 3, 3, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 3), (oh + kh), (ow + kw), floormod(ic, 3)]*p1[oc_chunk, floordiv(ic, 3), kh, kw, floormod(ic, 3), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"b8a5e3033c89946ea6e6b200a8131758\", [1, 16, 22, 92, 4], [1, 16, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 22, 92, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 21) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 91) - (ax3*5)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"a803fd172173aafe8b2479746b0de800\", [1, 64, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4608f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"7ab306678014d85d0d529e34f8888440\", [1, 1, 64, 22, 92], [1, 1, 64, 20, 90]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 64, 22, 92]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, (ax2 + rv0), (ax3 + rv1), (ax4 + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 20, 90, 4], [8, 16, 1, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [8, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 3, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 21)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 1, 3, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 91)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 8, 20, 90, 4], [32, 8, 1, 1, 4, 4], [1, 32, 1, 1, 4], [1, 32, 1, 1, 4], [1, 32, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [32, 8, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"3548697aabbe77f0c8f45e66184e6fa2\", [1, 32, 20, 90, 4], [1, 32, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 32, 20, 90, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 19) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 89) - (ax3*5)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"c74f956cb4d969f396ed6591ac94d329\", [1, 128, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 128, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/9216f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"82c9323a6bf75f7be33236a5c2cf52cb\", [1, 1, 128, 20, 90], [1, 1, 64, 18, 44]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 128, 20, 90]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*2) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 18, 44, 4], [16, 16, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 3, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 19)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 1, 3, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 45)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 18, 44, 4], [64, 16, 1, 1, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 64, 18, 44, 4], [16, 64, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 64, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"58bc26dbe1c1ee728a7795a35ebc9164\", [1, 64, 18, 44, 4], [1, 64, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*4) + rv0), ((ax3*2) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*4) + 3), 17) - (ax2*4)) + 1)*((min(((ax3*2) + 9), 43) - (ax3*2)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"00c8e3d0e14a873b682db24bd0733e7a\", [1, 256, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 256, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/18432f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"e6cc5ce637f8261c6a90a4b77c01efcd\", [1, 1, 256, 18, 44], [1, 1, 64, 16, 21]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 256, 18, 44]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*4) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 16, 21, 4], [64, 16, 1, 4, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 16, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 16, 21, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 4, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 64, 16, 18, 4], [17, 64, 13, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 16, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 64, 13, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"4bcb21a6c9411e8d23ad03b58c9dc753\", [1, 68, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 68, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4896f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"e1aec9a12f0e4438d7aba8a35a38d2b8\", [1, 129, 4, 18, 4], [17, 129, 1, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 129, 4, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 129, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"a000002b31f3eb26c291dab38d3199e1\", [1, 17, 4, 18, 4], [1, 17, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 17, 4, 18, 4]\n",
            "p0_red(ax0, ax1, ax2, ax3) += p0[ax0, ax1, k2, ax2, ax3]\n",
            "T_divide(ax0, ax1, ax2, ax3) = (p0_red[ax0, ax1, ax2, ax3]/4f)\n",
            "\n",
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  34.9130      29.6344      45.4792      29.6253       7.4714                  \n",
            "[Info] Test Accuracy: 0.894 [894:69:37:1000]\n",
            "[Info] Test Speed: 0.08546807956695557s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned + MLC"
      ],
      "metadata": {
        "id": "v0SkWW_A98cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test script to verify setup\n",
        "\n",
        "!python test_LPRNetOpt_mlc.py --test_img_dirs 'data/test' --optimized_model '/content/LPRNet_Optimization/weights/unstructured_pruned_LPRNet_model_0.5.pth' --log_file '/content/LPRNet_Pytorch/tuning_log.json'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLwk-bkk9-dU",
        "outputId": "d68da4dd-c1fc-432b-e899-d86e235ec11a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Compile...\n",
            "/content/LPRNet_Optimization/LPRNet_Optimization/LPRNet_Optimization/mlc/load_autotune.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(model_path, map_location = torch.device('cpu')))\n",
            "/content/LPRNet_Pytorch/tuning_log.json does not exist!\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1d64080c9dd43d925442e0ac42af9f21\", [1, 1, 24, 94, 3], [16, 1, 3, 3, 3, 4], [1, 16, 1, 1, 4], [1, 16, 1, 1, 4], [1, 16, 22, 92, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 24, 94, 3]\n",
            "p1 = PLACEHOLDER [16, 1, 3, 3, 3, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 3), (oh + kh), (ow + kw), floormod(ic, 3)]*p1[oc_chunk, floordiv(ic, 3), kh, kw, floormod(ic, 3), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"b8a5e3033c89946ea6e6b200a8131758\", [1, 16, 22, 92, 4], [1, 16, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 22, 92, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 21) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 91) - (ax3*5)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"a803fd172173aafe8b2479746b0de800\", [1, 64, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4608f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"7ab306678014d85d0d529e34f8888440\", [1, 1, 64, 22, 92], [1, 1, 64, 20, 90]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 64, 22, 92]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, (ax2 + rv0), (ax3 + rv1), (ax4 + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 20, 90, 4], [8, 16, 1, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [8, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 3, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 21)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 1, 3, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 91)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 8, 20, 90, 4], [32, 8, 1, 1, 4, 4], [1, 32, 1, 1, 4], [1, 32, 1, 1, 4], [1, 32, 20, 90, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [32, 8, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"3548697aabbe77f0c8f45e66184e6fa2\", [1, 32, 20, 90, 4], [1, 32, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 32, 20, 90, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 19) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 89) - (ax3*5)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"c74f956cb4d969f396ed6591ac94d329\", [1, 128, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 128, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/9216f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"82c9323a6bf75f7be33236a5c2cf52cb\", [1, 1, 128, 20, 90], [1, 1, 64, 18, 44]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 128, 20, 90]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*2) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 18, 44, 4], [16, 16, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 3, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 19)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 1, 3, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 45)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 18, 44, 4], [64, 16, 1, 1, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 64, 18, 44, 4], [16, 64, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 64, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_avg_pool2d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"58bc26dbe1c1ee728a7795a35ebc9164\", [1, 64, 18, 44, 4], [1, 64, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*4) + rv0), ((ax3*2) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*4) + 3), 17) - (ax2*4)) + 1)*((min(((ax3*2) + 9), 43) - (ax3*2)) + 1))))\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"00c8e3d0e14a873b682db24bd0733e7a\", [1, 256, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 256, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/18432f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_max_pool3d\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"e6cc5ce637f8261c6a90a4b77c01efcd\", [1, 1, 256, 18, 44], [1, 1, 64, 16, 21]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 1, 256, 18, 44]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*4) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 16, 21, 4], [64, 16, 1, 4, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 16, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 16, 16, 21, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 4, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"092e60abc4d1a2d8dae57703563611ae\", [1, 64, 16, 18, 4], [17, 64, 13, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 64, 16, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 64, 13, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"4bcb21a6c9411e8d23ad03b58c9dc753\", [1, 68, 4, 18], []]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 68, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4896f)\n",
            "\n",
            "-----------------------------------\n",
            "fused_nn_contrib_conv2d_NCHWc_add\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"e1aec9a12f0e4438d7aba8a35a38d2b8\", [1, 129, 4, 18, 4], [17, 129, 1, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 129, 4, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 129, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "\n",
            "-----------------------------------\n",
            "fused_mean\n",
            "Cannot find tuned schedules for target=llvm -keys=cpu -mtriple=x86_64-redhat-linux-gnu, workload_key=[\"a000002b31f3eb26c291dab38d3199e1\", [1, 17, 4, 18, 4], [1, 17, 18, 4]]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
            "p0 = PLACEHOLDER [1, 17, 4, 18, 4]\n",
            "p0_red(ax0, ax1, ax2, ax3) += p0[ax0, ax1, k2, ax2, ax3]\n",
            "T_divide(ax0, ax1, ax2, ax3) = (p0_red[ax0, ax1, ax2, ax3]/4f)\n",
            "\n",
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  26.5427      26.6055      26.8847      26.1380       0.3081                  \n",
            "[Info] Test Accuracy: 0.8 [800:123:77:1000]\n",
            "[Info] Test Speed: 0.06893685674667359s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create AutoScheduling from scratch"
      ],
      "metadata": {
        "id": "SsVXZxAJgoMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tvm\n",
        "from tvm import relay, autotvm, auto_scheduler\n",
        "from tvm.contrib import graph_executor, relay_viz\n",
        "from model.LPRNet import build_lprnet\n",
        "\n",
        "\n",
        "lprnet = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "lprnet.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "lprnet.eval()\n",
        "\n",
        "#quantized model\n",
        "# lprnet_quantize = build_lprnet_quantize(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "# lprnet_quantize.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Quantized_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "# lprnet_quantize.eval()\n",
        "# lprnet = quantized_model\n",
        "# model_path = 'weights/Final_LPRNet_model.pth'\n",
        "# model = load_pretrained_model(model_path)\n",
        "# model = QuantizedLPRNet(model)\n",
        "# # Define the quantization configuration\n",
        "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# # Prepare the model for quantization\n",
        "# model_prepared = torch.ao.quantization.prepare(model, inplace = False)\n",
        "\n",
        "# # Calibrate the model using a representative dataset\n",
        "# model_prepared.eval()\n",
        "# # Convert to quantized version\n",
        "# model_quantized = torch.ao.quantization.convert(model_prepared, inplace = False)\n",
        "# model_quantized.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Quantized_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "\n",
        "# Define an example input\n",
        "input_shape = (1, 3, 24, 94)\n",
        "input_data = torch.randn(input_shape)\n",
        "\n",
        "# Convert the model to a TorchScript module\n",
        "scripted_model = torch.jit.trace(lprnet, input_data)\n",
        "\n",
        "# Define the input shapes for TVM\n",
        "input_name = \"data\"\n",
        "shape_list = [(input_name, input_shape)]\n",
        "\n",
        "# Convert PyTorch model to Relay (high-level IR in TVM)\n",
        "mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)\n",
        "\n",
        "# Compile to TensorIR (low-level IR in TVM)\n",
        "target = \"llvm\"\n",
        "log_file = \"tuning_log.json\"\n",
        "\n",
        "print(\"Extract tasks...\")\n",
        "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n",
        "\n",
        "for idx, task in enumerate(tasks):\n",
        "    print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
        "    print(task.compute_dag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3-6HD_cCscC",
        "outputId": "84d97e7e-a6f7-4699-db06-b4eb64a15dcf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-1b2fa640db0f>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load('/content/LPRNet_Optimization/weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract tasks...\n",
            "========== Task 0  (workload key: [\"82c9323a6bf75f7be33236a5c2cf52cb\", [1, 1, 128, 20, 90], [1, 1, 64, 18, 44]]) ==========\n",
            "p0 = PLACEHOLDER [1, 1, 128, 20, 90]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*2) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "========== Task 1  (workload key: [\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 20, 90, 4], [8, 16, 1, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [8, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 2  (workload key: [\"58bc26dbe1c1ee728a7795a35ebc9164\", [1, 64, 18, 44, 4], [1, 64, 4, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*4) + rv0), ((ax3*2) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*4) + 3), 17) - (ax2*4)) + 1)*((min(((ax3*2) + 9), 43) - (ax3*2)) + 1))))\n",
            "\n",
            "========== Task 3  (workload key: [\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 1, 3, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 45)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 4  (workload key: [\"092e60abc4d1a2d8dae57703563611ae\", [1, 8, 20, 90, 4], [32, 8, 1, 1, 4, 4], [1, 32, 1, 1, 4], [1, 32, 1, 1, 4], [1, 32, 20, 90, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "p1 = PLACEHOLDER [32, 8, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 32, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 5  (workload key: [\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 3, 1, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 21)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 6  (workload key: [\"a000002b31f3eb26c291dab38d3199e1\", [1, 17, 4, 18, 4], [1, 17, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 17, 4, 18, 4]\n",
            "p0_red(ax0, ax1, ax2, ax3) += p0[ax0, ax1, k2, ax2, ax3]\n",
            "T_divide(ax0, ax1, ax2, ax3) = (p0_red[ax0, ax1, ax2, ax3]/4f)\n",
            "\n",
            "========== Task 7  (workload key: [\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 64, 18, 44, 4], [16, 64, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 64, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 64, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 8  (workload key: [\"00c8e3d0e14a873b682db24bd0733e7a\", [1, 256, 4, 18], []]) ==========\n",
            "p0 = PLACEHOLDER [1, 256, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/18432f)\n",
            "\n",
            "========== Task 9  (workload key: [\"3548697aabbe77f0c8f45e66184e6fa2\", [1, 32, 20, 90, 4], [1, 32, 4, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 32, 20, 90, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 19) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 89) - (ax3*5)) + 1))))\n",
            "\n",
            "========== Task 10  (workload key: [\"e1aec9a12f0e4438d7aba8a35a38d2b8\", [1, 129, 4, 18, 4], [17, 129, 1, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 129, 4, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 129, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "\n",
            "========== Task 11  (workload key: [\"092e60abc4d1a2d8dae57703563611ae\", [1, 64, 16, 18, 4], [17, 64, 13, 1, 4, 4], [1, 17, 1, 1, 4], [1, 17, 1, 1, 4], [1, 17, 4, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 64, 16, 18, 4]\n",
            "p1 = PLACEHOLDER [17, 64, 13, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 17, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 12  (workload key: [\"1f1aea4f3b33b88bd85488c01248041c\", [1, 8, 20, 90, 4], [8, 8, 1, 3, 4, 4], [1, 8, 1, 1, 4], [1, 8, 20, 90, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 8, 20, 90, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i3 >= 1) && (i3 < 91)), p0[i0, i1, i2, (i3 - 1), i4], 0f)\n",
            "p1 = PLACEHOLDER [8, 8, 1, 3, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 8, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 13  (workload key: [\"1d64080c9dd43d925442e0ac42af9f21\", [1, 1, 24, 94, 3], [16, 1, 3, 3, 3, 4], [1, 16, 1, 1, 4], [1, 16, 1, 1, 4], [1, 16, 22, 92, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 1, 24, 94, 3]\n",
            "p1 = PLACEHOLDER [16, 1, 3, 3, 3, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 3), (oh + kh), (ow + kw), floormod(ic, 3)]*p1[oc_chunk, floordiv(ic, 3), kh, kw, floormod(ic, 3), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 14  (workload key: [\"7ab306678014d85d0d529e34f8888440\", [1, 1, 64, 22, 92], [1, 1, 64, 20, 90]]) ==========\n",
            "p0 = PLACEHOLDER [1, 1, 64, 22, 92]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, (ax2 + rv0), (ax3 + rv1), (ax4 + rv2)]\n",
            "\n",
            "========== Task 15  (workload key: [\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 18, 44, 4], [64, 16, 1, 1, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 18, 44, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 16  (workload key: [\"b8a5e3033c89946ea6e6b200a8131758\", [1, 16, 22, 92, 4], [1, 16, 4, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 22, 92, 4]\n",
            "pool_sum(ax0, ax1, ax2, ax3, ax4) += p0[ax0, ax1, ((ax2*5) + rv0), ((ax3*5) + rv1), ax4]\n",
            "pool_avg(ax0, ax1, ax2, ax3, ax4) = (pool_sum[ax0, ax1, ax2, ax3, ax4]/float32((((min(((ax2*5) + 4), 21) - (ax2*5)) + 1)*((min(((ax3*5) + 4), 91) - (ax3*5)) + 1))))\n",
            "\n",
            "========== Task 17  (workload key: [\"ddfc1ebe8806378ca6f78e8a1f6be501\", [1, 16, 18, 44, 4], [16, 16, 1, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "p1 = PLACEHOLDER [16, 16, 1, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 18  (workload key: [\"e6cc5ce637f8261c6a90a4b77c01efcd\", [1, 1, 256, 18, 44], [1, 1, 64, 16, 21]]) ==========\n",
            "p0 = PLACEHOLDER [1, 1, 256, 18, 44]\n",
            "pool_max(ax0, ax1, ax2, ax3, ax4) max= p0[ax0, ax1, ((ax2*4) + rv0), (ax3 + rv1), ((ax4*2) + rv2)]\n",
            "\n",
            "========== Task 19  (workload key: [\"4bcb21a6c9411e8d23ad03b58c9dc753\", [1, 68, 4, 18], []]) ==========\n",
            "p0 = PLACEHOLDER [1, 68, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4896f)\n",
            "\n",
            "========== Task 20  (workload key: [\"c74f956cb4d969f396ed6591ac94d329\", [1, 128, 4, 18], []]) ==========\n",
            "p0 = PLACEHOLDER [1, 128, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/9216f)\n",
            "\n",
            "========== Task 21  (workload key: [\"1f1aea4f3b33b88bd85488c01248041c\", [1, 16, 18, 44, 4], [16, 16, 3, 1, 4, 4], [1, 16, 1, 1, 4], [1, 16, 18, 44, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 18, 44, 4]\n",
            "data_pad(i0, i1, i2, i3, i4) = tir.if_then_else(((i2 >= 1) && (i2 < 19)), p0[i0, i1, (i2 - 1), i3, i4], 0f)\n",
            "p1 = PLACEHOLDER [16, 16, 3, 1, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (data_pad[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 16, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n",
            "========== Task 22  (workload key: [\"a803fd172173aafe8b2479746b0de800\", [1, 64, 4, 18], []]) ==========\n",
            "p0 = PLACEHOLDER [1, 64, 4, 18]\n",
            "p0_red() += p0[k0, k1, k2, k3]\n",
            "T_divide() = (p0_red[]/4608f)\n",
            "\n",
            "========== Task 23  (workload key: [\"092e60abc4d1a2d8dae57703563611ae\", [1, 16, 16, 21, 4], [64, 16, 1, 4, 4, 4], [1, 64, 1, 1, 4], [1, 64, 1, 1, 4], [1, 64, 16, 18, 4]]) ==========\n",
            "p0 = PLACEHOLDER [1, 16, 16, 21, 4]\n",
            "p1 = PLACEHOLDER [64, 16, 1, 4, 4, 4]\n",
            "conv2d_NCHWc(n, oc_chunk, oh, ow, oc_block) += (p0[n, floordiv(ic, 4), (oh + kh), (ow + kw), floormod(ic, 4)]*p1[oc_chunk, floordiv(ic, 4), kh, kw, floormod(ic, 4), oc_block])\n",
            "p2 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4])\n",
            "p3 = PLACEHOLDER [1, 64, 1, 1, 4]\n",
            "T_add(ax0, ax1, ax2, ax3, ax4) = (T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4])\n",
            "T_relu(ax0, ax1, ax2, ax3, ax4) = max(T_add[ax0, ax1, ax2, ax3, ax4], 0f)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
        "tune_option = auto_scheduler.TuningOptions(\n",
        "    num_measure_trials=200,  # change this to 20000 to achieve the best performance\n",
        "    runner=auto_scheduler.LocalRunner(repeat=10, enable_cpu_cache_flush=True),\n",
        "    measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
        ")\n",
        "\n",
        "tuner.tune(tune_option)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLXQucnYrpLi",
        "outputId": "334f539b-b95d-4151-be1d-356bd58dce65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |            - |              - |      0 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |            - |              - |      0 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |            - |              - |      0 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |            - |              - |      0 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |            - |              - |      0 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |            - |              - |      0 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |            - |              - |      0 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 8\tUsed time : 59 s\tNext ID: 1\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |            - |              - |      0 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |            - |              - |      0 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 16\tUsed time : 184 s\tNext ID: 2\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |            - |              - |      0 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 24\tUsed time : 281 s\tNext ID: 3\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 32\tUsed time : 483 s\tNext ID: 4\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |            - |              - |      0 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 40\tUsed time : 622 s\tNext ID: 5\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |            - |              - |      0 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 48\tUsed time : 820 s\tNext ID: 6\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |            - |              - |      0 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 56\tUsed time : 894 s\tNext ID: 7\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |            - |              - |      0 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 64\tUsed time : 1027 s\tNext ID: 8\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 72\tUsed time : 1063 s\tNext ID: 9\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |            - |              - |      0 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 80\tUsed time : 1159 s\tNext ID: 10\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 88\tUsed time : 1275 s\tNext ID: 11\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 96\tUsed time : 1404 s\tNext ID: 12\t\n",
            "\n",
            ".T\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |            - |              - |      0 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 104\tUsed time : 1617 s\tNext ID: 13\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 112\tUsed time : 1755 s\tNext ID: 14\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |            - |              - |      0 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 120\tUsed time : 1819 s\tNext ID: 15\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 128\tUsed time : 1955 s\tNext ID: 16\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |            - |              - |      0 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 136\tUsed time : 2048 s\tNext ID: 17\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 144\tUsed time : 2177 s\tNext ID: 18\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |            - |              - |      0 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 152\tUsed time : 2241 s\tNext ID: 19\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |        0.001 |           4.82 |      8 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |            - |              - |      0 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 160\tUsed time : 2277 s\tNext ID: 20\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |        0.001 |           4.82 |      8 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |        0.001 |           9.47 |      8 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |            - |              - |      0 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 168\tUsed time : 2314 s\tNext ID: 21\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |        0.001 |           4.82 |      8 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |        0.001 |           9.47 |      8 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |        2.206 |           8.87 |      8 |\n",
            "|   22 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 176\tUsed time : 2516 s\tNext ID: 22\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |        0.001 |           4.82 |      8 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |        0.001 |           9.47 |      8 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |        2.206 |           8.87 |      8 |\n",
            "|   22 |                                             vm_mod_fused_mean |        0.000 |          10.82 |      8 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |            - |              - |      0 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: - ms\tTrials: 184\tUsed time : 2550 s\tNext ID: 23\t\n",
            "\n",
            "\n",
            "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "|    0 |                                  vm_mod_fused_nn_max_pool3d_1 |        0.142 |           3.21 |      8 |\n",
            "|    1 |              vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu |        1.350 |           5.55 |      8 |\n",
            "|    2 |                                  vm_mod_fused_nn_avg_pool2d_2 |        0.128 |           5.91 |      8 |\n",
            "|    3 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |        1.372 |          14.26 |      8 |\n",
            "|    4 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |        1.257 |          12.28 |      8 |\n",
            "|    5 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |        1.925 |           5.80 |      8 |\n",
            "|    6 |                                           vm_mod_fused_mean_4 |        0.002 |           3.31 |      8 |\n",
            "|    7 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |        2.213 |          11.77 |      8 |\n",
            "|    8 |                                           vm_mod_fused_mean_2 |        0.009 |           2.00 |      8 |\n",
            "|    9 |                                  vm_mod_fused_nn_avg_pool2d_1 |        0.047 |           5.12 |      8 |\n",
            "|   10 |                      vm_mod_fused_nn_contrib_conv2d_NCHWc_add |        0.489 |          10.34 |      8 |\n",
            "|   11 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_4 |        4.105 |           7.94 |      8 |\n",
            "|   12 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |        1.413 |           7.91 |      8 |\n",
            "|   13 |          vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |        0.865 |           8.54 |      8 |\n",
            "|   14 |                                    vm_mod_fused_nn_max_pool3d |        0.125 |           8.33 |      8 |\n",
            "|   15 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |        3.273 |           8.12 |      8 |\n",
            "|   16 |                                    vm_mod_fused_nn_avg_pool2d |        0.028 |           4.24 |      8 |\n",
            "|   17 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |        0.831 |           7.93 |      8 |\n",
            "|   18 |                                  vm_mod_fused_nn_max_pool3d_2 |        0.044 |           4.35 |      8 |\n",
            "|   19 |                                           vm_mod_fused_mean_3 |        0.001 |           4.82 |      8 |\n",
            "|   20 |                                           vm_mod_fused_mean_1 |        0.001 |           9.47 |      8 |\n",
            "|   21 |            vm_mod_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |        2.206 |           8.87 |      8 |\n",
            "|   22 |                                             vm_mod_fused_mean |        0.000 |          10.82 |      8 |\n",
            "|   23 |        vm_mod_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |        5.055 |           7.51 |      8 |\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "Estimated total latency: 33.730 ms\tTrials: 192\tUsed time : 2707 s\tNext ID: 15\t\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Types of Optimizations:\n",
        "\n",
        "**Operator Fusion**: Combining multiple operations (e.g., convolution + bias add + relu, or sum + divide for mean) into a single kernel to reduce memory traffic.\n",
        "\n",
        "**Data Layout Transformation (NCHWc)**: Changing the tensor layout to improve memory locality, enable vectorization, and better utilize hardware capabilities.\n",
        "\n",
        "**Loop-Level Optimizations (Tiling, Vectorization, Parallelization)**:\n",
        "Tiling: Dividing loops into smaller blocks to improve cache locality.\n",
        "\n",
        "**Vectorization**: Using SIMD instructions for parallel processing of multiple data elements simultaneously.\n",
        "\n",
        "**Parallelization**: Distributing computation across multiple CPU cores or GPU threads.\n",
        "\n",
        "**Reduction Optimization**: Efficiently handling pooling and mean (averaging) operations by fusing reductions and using tiling/vectorization for speed."
      ],
      "metadata": {
        "id": "5aeKLxeklS3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Compile with the history best\n",
        "print(\"Compile...\")\n",
        "with auto_scheduler.ApplyHistoryBest(log_file):\n",
        "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
        "        lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "# Create graph executor\n",
        "dev = tvm.device(str(target), 0)\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(\"float32\"))\n",
        "module.set_input(\"data\", data_tvm)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluate inference time cost...\")\n",
        "print(module.benchmark(dev, repeat=3, min_repeat_ms=500))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkyNR9TwrmSI",
        "outputId": "ab682a37-05f9-4016-b721-09e4999c285c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compile...\n",
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  34.3033      34.4002      34.6132      33.8965       0.3005                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Inference test"
      ],
      "metadata": {
        "id": "-zWKSGhylqxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare original and optimized models\n",
        "import time\n",
        "\n",
        "def pytorch_inference(model, input_data, num_runs=100):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            _ = model(input_data)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "def tvm_inference(lib, dev, input_data, num_runs=100):\n",
        "    module = tvm.contrib.graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "    module.set_input(input_name, tvm.nd.array(input_data.numpy()))\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        module.run()\n",
        "        _ = module.get_output(0)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "# Run comparisons\n",
        "pytorch_time = pytorch_inference(model, input_data)\n",
        "dev = tvm.cpu(0)\n",
        "tvm_time = tvm_inference(lib, dev, input_data)\n",
        "\n",
        "print(f\"PyTorch average inference time: {pytorch_time:.6f} seconds\")\n",
        "print(f\"TVM optimized average inference time: {tvm_time:.6f} seconds\")\n",
        "print(f\"Speedup: {pytorch_time / tvm_time:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRtvl2Ad5Mm7",
        "outputId": "9ab7e407-a37c-4743-9923-8a7c779ca9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch average inference time: 0.269682 seconds\n",
            "TVM optimized average inference time: 0.046423 seconds\n",
            "Speedup: 5.81x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model performance on Test data"
      ],
      "metadata": {
        "id": "zHIUzrNZlwH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from model.LPRNet import build_lprnet\n",
        "# import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
        "\n",
        "\n",
        "def test(model_type):\n",
        "  test_img_dirs = os.path.expanduser('/content/LPRNet_Pytorch/data/test')\n",
        "  datasets = LPRDataLoader(test_img_dirs.split(','), [94, 24], 8)\n",
        "\n",
        "  epoch_size = len(datasets) // 1\n",
        "  batch_iterator = iter(DataLoader(datasets, 1, shuffle=True, num_workers=8, collate_fn=collate_fn))\n",
        "  Tp = 0\n",
        "  Tn_1 = 0\n",
        "  Tn_2 = 0\n",
        "  t1 = time.time()\n",
        "  for i in range(epoch_size):\n",
        "      # load train data\n",
        "      images, labels, lengths = next(batch_iterator)\n",
        "      start = 0\n",
        "      targets = []\n",
        "      for length in lengths:\n",
        "          label = labels[start:start+length]\n",
        "          targets.append(label)\n",
        "          start += length\n",
        "      targets = np.array([el.numpy() for el in targets])\n",
        "      imgs = images.numpy().copy()\n",
        "\n",
        "      images = Variable(images)\n",
        "      if model_type == 'pytorch':\n",
        "        prebs = quantized_model(images)\n",
        "        prebs = prebs.cpu().detach().numpy()\n",
        "      else:\n",
        "        module.set_input(\"data\", tvm.nd.array(images.numpy()))\n",
        "\n",
        "        # Run inference\n",
        "        module.run()\n",
        "\n",
        "        # Get output\n",
        "        prebs = module.get_output(0).asnumpy()\n",
        "      # greedy decode\n",
        "      preb_labels = list()\n",
        "      for i in range(prebs.shape[0]):\n",
        "          preb = prebs[i, :, :]\n",
        "          preb_label = list()\n",
        "          for j in range(preb.shape[1]):\n",
        "              preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "          no_repeat_blank_label = list()\n",
        "          pre_c = preb_label[0]\n",
        "          if pre_c != len(CHARS) - 1:\n",
        "              no_repeat_blank_label.append(pre_c)\n",
        "          for c in preb_label: # dropout repeate label and blank label\n",
        "              if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                  if c == len(CHARS) - 1:\n",
        "                      pre_c = c\n",
        "                  continue\n",
        "              no_repeat_blank_label.append(c)\n",
        "              pre_c = c\n",
        "          preb_labels.append(no_repeat_blank_label)\n",
        "      for i, label in enumerate(preb_labels):\n",
        "          if len(label) != len(targets[i]):\n",
        "              Tn_1 += 1\n",
        "              continue\n",
        "          if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "              Tp += 1\n",
        "          else:\n",
        "              Tn_2 += 1\n",
        "  Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "  print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "  t2 = time.time()\n",
        "  print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
      ],
      "metadata": {
        "id": "OYBiFd4L5MkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model_type= \"tvm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zNimk-_5MfH",
        "outputId": "60c99b3f-2ff2-4b27-ae90-36e88aa8720d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.894 [894:69:37:1000]\n",
            "[Info] Test Speed: 0.049105304479599s 1/1000]\n"
          ]
        }
      ]
    }
  ]
}